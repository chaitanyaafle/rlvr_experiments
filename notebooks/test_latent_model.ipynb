{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Latent Qwen Model\n",
        "\n",
        "This notebook allows you to manually verify the `LatentQwen2ForCausalLM` definition."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "from latent_qwen import LatentQwen2ForCausalLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Model and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: Qwen/Qwen2-0.5B-Instruct\n",
            "Loading Latent Model...\n",
            "IDs: <think>=151646, </think>=151647, <answer>=151648\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen2-0.5B-Instruct\"\n",
        "num_latent_thoughts = 4\n",
        "\n",
        "print(f\"Loading model: {model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Add special tokens\n",
        "special_tokens = [\"<think>\", \"</think>\", \"<answer>\", \"</answer>\"]\n",
        "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
        "\n",
        "print(\"Loading Latent Model...\")\n",
        "model = LatentQwen2ForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    num_latent_thoughts=num_latent_thoughts,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=\"auto\"\n",
        ")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set Token IDs\n",
        "think_id = tokenizer.convert_tokens_to_ids(\"<think>\")\n",
        "close_think_id = tokenizer.convert_tokens_to_ids(\"</think>\")\n",
        "answer_id = tokenizer.convert_tokens_to_ids(\"<answer>\")\n",
        "\n",
        "model.set_special_token_ids(think_id)\n",
        "model.close_think_id = close_think_id\n",
        "model.answer_id = answer_id\n",
        "\n",
        "print(f\"IDs: <think>={think_id}, </think>={close_think_id}, <answer>={answer_id}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Test Forward Pass\n",
        "We check if the forward pass accepts input with `<think>` and processes it without crashing. Note that the output logits have the expanded length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input IDs: tensor([[  1474,     25,  63284,    220,     17,     10,     17,     13,    220,\n",
            "         151646]], device='cuda:0')\n",
            "Logits Shape: torch.Size([1, 14, 151650])\n",
            "Expected Shape: (1, 14, 151650)\n"
          ]
        }
      ],
      "source": [
        "text = \"User: Solve 2+2. <think>\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(f\"Input IDs: {inputs.input_ids}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "logits = outputs.logits\n",
        "print(f\"Logits Shape: {logits.shape}\")\n",
        "\n",
        "# Verify shape\n",
        "# If <think> is present, the model expands the sequence by `num_latent_thoughts`\n",
        "expected_len = inputs.input_ids.shape[1]\n",
        "if \"<think>\" in text:\n",
        "    expected_len += num_latent_thoughts\n",
        "\n",
        "print(f\"Expected Shape: {(inputs.input_ids.shape[0], expected_len, model.config.vocab_size)}\")\n",
        "\n",
        "assert logits.shape[1] == expected_len, f\"Output shape mismatch! Got {logits.shape[1]}, expected {expected_len}\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Test Generation\n",
        "We test if the model automatically produces `</think>` after `<think>` due to our forced generation logic."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated IDs: tensor([  1474,     25,   3555,    374,    220,     18,     10,     18,     30,\n",
            "           220, 151646, 151647,    271,     40,   2776,     68,   3970,    279,\n",
            "          4226,    304,    264], device='cuda:0')\n",
            "Generated Text: User: What is 3+3? <think></think>\n",
            "\n",
            "I'measure the answer in a\n",
            "SUCCESS: </think> found in output.\n"
          ]
        }
      ],
      "source": [
        "prompt = \"User: What is 3+3? <think>\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "# Generate\n",
        "# We generate a few tokens. Expected: </think> ...\n",
        "output_ids = model.generate(\n",
        "    inputs.input_ids,\n",
        "    max_new_tokens=10,\n",
        "    do_sample=False # Greedy\n",
        ")\n",
        "\n",
        "print(\"Generated IDs:\", output_ids[0])\n",
        "decoded = tokenizer.decode(output_ids[0], skip_special_tokens=False)\n",
        "print(\"Generated Text:\", decoded)\n",
        "\n",
        "# Validation\n",
        "if \"</think>\" in decoded:\n",
        "    print(\"SUCCESS: </think> found in output.\")\n",
        "else:\n",
        "    print(\"FAILURE: </think> NOT found.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
