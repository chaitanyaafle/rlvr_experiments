{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Maze Environment Visualization and Model Demo\n",
                "\n",
                "This notebook demonstrates how to:\n",
                "1. Load the Maze environment configuration.\n",
                "2. Generate examples from the environment.\n",
                "3. Visualize the raw prompts sent to the model.\n",
                "4. Run the model to see its formatted output."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loaded config for environment: maze\n"
                    ]
                }
            ],
            "source": [
                "import yaml\n",
                "import torch\n",
                "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
                "from environments.maze_env import MazeEnvironment\n",
                "\n",
                "# 1. Load Configuration\n",
                "config_path = '../configs/config_maze.yaml'\n",
                "with open(config_path, 'r') as f:\n",
                "    config = yaml.safe_load(f)\n",
                "\n",
                "print(f\"Loaded config for environment: {config['environment']['name']}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Generating Maze dataset with config: MazeConfig(min_dist=5, max_dist=10, min_grid_size=5, max_grid_size=10, seed=42, size=500)\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Map: 100%|██████████| 500/500 [00:00<00:00, 12175.90 examples/s]"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset generated with 500 examples.\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "source": [
                "# 2. Initialize Environment and Dataset\n",
                "env = MazeEnvironment(config)\n",
                "dataset = env.get_dataset(config)\n",
                "\n",
                "print(f\"Dataset generated with {len(dataset)} examples.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "--- Sample Index: 136 ---\n",
                        "\n",
                        "System Prompt:\n",
                        "You are a helpful assistant. You must output your reasoning steps within <think></think> tags and your final answer within <answer></answer> tags.\n",
                        "            You goal is to find the shortest path from start to end, you can move steps in passage, while avoiding the Walls.\n",
                        "\n",
                        "User Prompt (The Maze):\n",
                        "Navigate from 'X' (start) to '?' (goal):\n",
                        "\n",
                        "```\n",
                        "eeeeee\n",
                        "ee*eee\n",
                        "eXee*e\n",
                        "e*e?ee\n",
                        "e****e\n",
                        "eeeeee\n",
                        "```\n",
                        "Legend: 'e' = Wall, '*' = Passage\n",
                        "\n",
                        "What is the minimum number of steps to reach the goal?\n",
                        "Give only the number of steps as your final answer, no other text or formatting.\n",
                        "\n",
                        "Ground Truth Answer:\n",
                        "5\n"
                    ]
                }
            ],
            "source": [
                "# 3. Visualize a Sample\n",
                "import random\n",
                "\n",
                "idx = random.randint(0, len(dataset)-1)\n",
                "sample = dataset[idx]\n",
                "\n",
                "print(f\"--- Sample Index: {idx} ---\\n\")\n",
                "print(\"System Prompt:\")\n",
                "print(sample['prompt'][0]['content'])\n",
                "print(\"\\nUser Prompt (The Maze):\")\n",
                "print(sample['prompt'][1]['content'])\n",
                "print(\"\\nGround Truth Answer:\")\n",
                "print(sample['answer'])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading model...\n",
                        "Model loaded.\n"
                    ]
                }
            ],
            "source": [
                "# 4. Load Model (Optional - can skip if no GPU or just want to see data)\n",
                "print(\"Loading model...\")\n",
                "model_name =\"/home/rlvr_experiments/outputs/maze-sft\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
                "model = AutoModelForCausalLM.from_pretrained(\n",
                "    model_name, \n",
                "    torch_dtype='auto', \n",
                "    device_map='auto'\n",
                ")\n",
                "print(\"Model loaded.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "[{'content': 'You are a helpful assistant. You must output your reasoning steps within <think></think> tags and your final answer within <answer></answer> tags.\\n            You goal is to find the shortest path from start to end, you can move steps in passage, while avoiding the Walls.',\n",
                            "  'role': 'system'},\n",
                            " {'content': \"Navigate from 'X' (start) to '?' (goal):\\n\\n```\\neeeeee\\nee*eee\\neXee*e\\ne*e?ee\\ne****e\\neeeeee\\n```\\nLegend: 'e' = Wall, '*' = Passage\\n\\nWhat is the minimum number of steps to reach the goal?\\nGive only the number of steps as your final answer, no other text or formatting.\",\n",
                            "  'role': 'user'}]"
                        ]
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "sample['prompt']"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Running inference on the sample above...\n",
                        "\n",
                        "<|im_start|>system\n",
                        "You are a helpful assistant. You must output your reasoning steps within <think></think> tags and your final answer within <answer></answer> tags.\n",
                        "            You goal is to find the shortest path from start to end, you can move steps in passage, while avoiding the Walls.<|im_end|>\n",
                        "<|im_start|>user\n",
                        "Navigate from 'X' (start) to '?' (goal):\n",
                        "\n",
                        "```\n",
                        "eeeeee\n",
                        "ee*eee\n",
                        "eXee*e\n",
                        "e*e?ee\n",
                        "e****e\n",
                        "eeeeee\n",
                        "```\n",
                        "Legend: 'e' = Wall, '*' = Passage\n",
                        "\n",
                        "What is the minimum number of steps to reach the goal?\n",
                        "<|im_end|>\n",
                        "<|im_start|>assistant\n",
                        "\n",
                        "--- Model Response ---\n",
                        "<think>Step 1: Find the starting point</step>\n",
                        "Starting at 'X', we need to move one step forward.</step>\n",
                        "<think>Step 2: Move right</step>\n",
                        "We have reached the next step.</step>\n",
                        "<think>Step 3: Move left</step>\n",
                        "We have reached the goal.</step>\n",
                        "<think>Step 4: Return to the starting point</step>\n",
                        "We have completed our journey.</step>\n",
                        "<think>Step 5: Calculate the total number of steps</step>\n",
                        "Total number of steps = Step 1 + Step 2 + Step 3 + Step 4</step>\n",
                        "<think>Step 6: Output the result</step>\n",
                        "Total number of steps = 4</step>\n",
                        "<think>Final Answer</step><answer>4</answer><|im_end|>\n"
                    ]
                }
            ],
            "source": [
                "# 5. Run Inference\n",
                "def generate_response(prompt_messages):\n",
                "\n",
                "    text = tokenizer.apply_chat_template(prompt_messages, tokenize=False, add_generation_prompt=True)\n",
                "    text = text.replace(\"Give only the number of steps as your final answer, no other text or formatting.\",\"\")\n",
                "\n",
                "    print(text)\n",
                "    inputs = tokenizer(text, \n",
                "    return_tensors='pt').to(model.device)\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model.generate(\n",
                "            **inputs, \n",
                "            max_new_tokens=config['generation']['max_completion_length'],\n",
                "            do_sample=False,\n",
                "        )\n",
                "    \n",
                "    # Decode only the new tokens\n",
                "    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=False)\n",
                "    return response\n",
                "\n",
                "print(\"Running inference on the sample above...\\n\")\n",
                "response = generate_response(sample['prompt'])\n",
                "\n",
                "print(\"--- Model Response ---\")\n",
                "print(response)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
